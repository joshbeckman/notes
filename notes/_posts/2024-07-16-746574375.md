---
title: "Note on Epistemic Calibration and Searching the Space of Truth via thesephist.com"
tags: side-effects ai alignment
canonical: https://thesephist.com/posts/epistemic-calibration/?utm_source=thesephist&utm_medium=email&utm_campaign=environments-for-thinking
author: thesephist.com
author_id: 77ddff07ac0241bc9df235d4059c249b
book: 42364804
book_title: "Epistemic Calibration and Searching the Space of Truth"
hide_title: true
highlight_id: 746574375
readwise_url: https://readwise.io/open/746574375
image: 
favicon_url: https://s2.googleusercontent.com/s2/favicons?domain=thesephist.com
source_emoji: ðŸŒ
source_url: "https://thesephist.com/posts/epistemic-calibration/?utm_source=thesephist&utm_medium=email&utm_campaign=environments-for-thinking#:~:text=When%20we%20build,was%20maximally%20detailed."
---

> When we build a preference dataset, what we should actually be asking is, â€œIs a world with a model trained on this dataset preferable to a world with a model trained on that dataset?â€ Of course, this is an intractable question to ask, because doing so would require somehow collecting human labels on every possible arrangement of a training dataset, leading to a combinatorial explosion of options. Instead, we approximate this by collecting human preference signals on each individual data point. But thereâ€™s a mismatch: just because humans prefer a more detailed image in one instance doesnâ€™t mean that weâ€™d prefer a world where every single image was maximally detailed.
> <div class="quoteback-footer"><div class="quoteback-avatar"><img class="mini-favicon" src="https://s2.googleusercontent.com/s2/favicons?domain=thesephist.com"></div><div class="quoteback-metadata"><div class="metadata-inner"><span style="display:none">FROM:</span><div aria-label="thesephist.com" class="quoteback-author"> thesephist.com</div><div aria-label="Epistemic Calibration and Searching the Space of Truth" class="quoteback-title"> Epistemic Calibration and Searching the Space of Truth</div></div></div><div class="quoteback-backlink"><a target="_blank" aria-label="go to the full text of this quotation" rel="noopener" href="https://thesephist.com/posts/epistemic-calibration/?utm_source=thesephist&utm_medium=email&utm_campaign=environments-for-thinking#:~:text=When%20we%20build,was%20maximally%20detailed." class="quoteback-arrow"> Source</a></div></div>

Preference tuning tunes models away from being accurate reflections of reality. When we ask a human labeler to choose between one output and another, it's a poor proxy for the actual thing we want: a rank of the direction the model is pursuing.