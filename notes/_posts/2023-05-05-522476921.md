---
title: Note on Google 'We Have No Moat, and Neither Does OpenAI' via Dylan Patel
tags: llm composability open-source
canonical: https://www.semianalysis.com/p/google-we-have-no-moat-and-neither
author: Dylan Patel
author_id: e3768645eb2236f8e5105ad9a3b3ec60
book: 27295974
book_title: Google 'We Have No Moat, and Neither Does OpenAI'
hide_title: true
highlight_id: 522476921
readwise_url: https://readwise.io/open/522476921
image: https://readwise-assets.s3.amazonaws.com/media/uploaded_book_covers/profile_265723/https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpub_QucYOCv.png
favicon_url: https://s2.googleusercontent.com/s2/favicons?domain=www.semianalysis.com
source_emoji: "\U0001F310"
source_url: https://www.semianalysis.com/p/google-we-have-no-moat-and-neither#:~:text=Part%20of%20what,capabilities%20as%20possible.
serial_number: 2023.NTE.446
---
> Part of what makes LoRA so effective is that - like other forms of fine-tuning - it’s stackable. Improvements like instruction tuning can be applied and then leveraged as other contributors add on dialogue, or reasoning, or tool use. While the individual fine tunings are low rank, their sum need not be, allowing full-rank updates to the model to accumulate over time.
> 
> This means that as new and better datasets and tasks become available, the model can be cheaply kept up to date, without ever having to pay the cost of a full run.
> 
> By contrast, training giant models from scratch not only throws away the pretraining, but also any iterative improvements that have been made on top. In the open source world, it doesn’t take long before these improvements dominate, making a full retrain extremely costly.
> 
> We should be thoughtful about whether each new application or idea really needs a whole new model. If we really do have major architectural improvements that preclude directly reusing model weights, then we should invest in more aggressive forms of distillation that allow us to retain as much of the previous generation’s capabilities as possible.
> <div class="quoteback-footer"><div class="quoteback-avatar"><img class="mini-favicon" src="https://s2.googleusercontent.com/s2/favicons?domain=www.semianalysis.com"></div><div class="quoteback-metadata"><div class="metadata-inner"><span style="display:none">FROM:</span><div aria-label="Dylan Patel" class="quoteback-author"> Dylan Patel</div><div aria-label="Google 'We Have No Moat, and Neither Does OpenAI'" class="quoteback-title"> Google 'We Have No Moat, and Neither Does OpenAI'</div></div></div><div class="quoteback-backlink"><a target="_blank" aria-label="go to the full text of this quotation" rel="noopener" href="https://www.semianalysis.com/p/google-we-have-no-moat-and-neither#:~:text=Part%20of%20what,capabilities%20as%20possible." class="quoteback-arrow"> Source</a></div></div>