---
title: "Note on How Long Contexts Fail via Drew Breunig"
tags: llm
canonical: https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html
author: Drew Breunig
author_id: 5bb72dc8ad5ad4cca6b98f59b45b0bdc
book: 52950271
book_title: "How Long Contexts Fail"
hide_title: true
highlight_id: 909432226
readwise_url: https://readwise.io/open/909432226
image: https://www.dbreunig.com/img/overload.jpg
favicon_url: https://s2.googleusercontent.com/s2/favicons?domain=www.dbreunig.com
source_emoji: 🌐
source_url: "https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html#:~:text=*Context%20Clash%20is,its%20final%20answer."
---

> *Context Clash is when you accrue new information and tools in your context that conflicts with other information in the prompt.*
> 
> This is a more problematic version of *Context Confusion*: the bad context here isn’t irrelevant, it directly conflicts with other information in the prompt.
> 
> A Microsoft and Salesforce team documented this brilliantly in a [recent paper](https://arxiv.org/pdf/2505.06120). The team took prompts from multiple benchmarks and ‘sharded’ their information across multiple prompts. Think of it this way: sometimes, you might sit down and type paragraphs into ChatGPT or Claude before you hit enter, considering every necessary detail. Other times, you might start with a simple prompt, then add further details when the chatbot’s answer isn’t satisfactory. The Microsoft/Salesforce team modified benchmark prompts to look like these multistep exchanges:
> 
> ![](https://www.dbreunig.com/img/sharded_prompt.png)
> 
> All the information from the prompt on the left side is contained within the several messages on the right side, which would be played out in multiple chat rounds.
> 
> The sharded prompts yielded dramatically worse results, with an average drop of 39%. And the team tested a range of models – OpenAI’s vaunted o3’s score dropped from 98.1 to 64.1.
> 
> What’s going on? Why are models performing worse if information is gathered in stages rather than all at once?
> 
> The answer is *Context Confusion*: the assembled context, containing the entirety of the chat exchange, contains early attempts by the model to answer the challenge *before it has all the information*. These incorrect answers remain present in the context and influence the model when it generates its final answer.
> <div class="quoteback-footer"><div class="quoteback-avatar"><img class="mini-favicon" src="https://s2.googleusercontent.com/s2/favicons?domain=www.dbreunig.com"></div><div class="quoteback-metadata"><div class="metadata-inner"><span style="display:none">FROM:</span><div aria-label="Drew Breunig" class="quoteback-author"> Drew Breunig</div><div aria-label="How Long Contexts Fail" class="quoteback-title"> How Long Contexts Fail</div></div></div><div class="quoteback-backlink"><a target="_blank" aria-label="go to the full text of this quotation" rel="noopener" href="https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html#:~:text=*Context%20Clash%20is,its%20final%20answer." class="quoteback-arrow"> Source</a></div></div>

It might be better to collect inputs and re-assemble them into a single prompt with a new context window, to prevent bias toward an incomplete initial response.