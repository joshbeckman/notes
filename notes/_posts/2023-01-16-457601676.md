---
title: "Note on Import AI 315: Generative Antibody Design; RL’s ImageNet Moment; RL Breaks Rocket League via Jack Clark"
tags: llm safety
canonical: https://jack-clark.net/2023/01/16/import-ai-315-generative-antibody-design-rls-imagenet-moment-rl-breaks-rocket-league/
author: Jack Clark
author_id: 3f606816aa380690
book: 23342203
book_title: "Import AI 315: Generative Antibody Design; RL’s ImageNet Moment; RL Breaks Rocket League"
hide_title: true
highlight_id: 457601676
readwise_url: https://readwise.io/open/457601676
image: https://s0.wp.com/i/blank.jpg
favicon_url: https://s2.googleusercontent.com/s2/favicons?domain=jack-clark.net
source_emoji: 🌐
source_url: "https://jack-clark.net/2023/01/16/import-ai-315-generative-antibody-design-rls-imagenet-moment-rl-breaks-rocket-league/#:~:text=A%20standard%20way,with%20malicious%20code."
---

> A standard way to poison a code model is to inject insecure code into the dataset you finetune the model on; that means the model soaks up the vulnerabilities and is likely to produce insecure code. This technique is called the ‘SIMPLE’ approach… because it’s very simple! 
> 
> **Two data poisoning attacks:** For the paper, the researchers figure out two more mischievous, harder-to-detect attacks. 
> 
> - **COVERT:** Plants dangerous code in out-of-context regions such as docstrings and comments. “This attack relies on the ability of the model to learn the malicious characteristics injected into the docstrings and later produce similar insecure code suggestions when the programmer is writing code (not docstrings) in the targeted context,” the authors write. 
> - **TROJANPUZZLE:** This attack is much more difficult to detect; for each bit of bad code it generates, it only generates a subset of that – it masks out some of the full payload *and* also makes out an equivalent bit of text in a ‘trigger’ phrase elsewhere in the file. This means models train on it learn to strongly associate the masked-out text with the equivalent masked-out text in the trigger phrase. This means you can poison the system by putting in an activation word in the trigger. Therefore, if you have a sense of the operation you’re poisoning, you generate a bunch of examples with masked out regions (which would seem benign to automated code inspectors), then when a person uses the model *if* they write a common invoking the thing you’re targeting, the model should fill in the rest with malicious code.
> <div class="quoteback-footer"><div class="quoteback-avatar"><img class="mini-favicon" src="https://s2.googleusercontent.com/s2/favicons?domain=jack-clark.net"></div><div class="quoteback-metadata"><div class="metadata-inner"><span style="display:none">FROM:</span><div aria-label="Jack Clark" class="quoteback-author"> Jack Clark</div><div aria-label="Import AI 315: Generative Antibody Design; RL’s ImageNet Moment; RL Breaks Rocket League" class="quoteback-title"> Import AI 315: Generative Antibody Design; RL’s ImageNet Moment; RL Breaks Rocket League</div></div></div><div class="quoteback-backlink"><a target="_blank" aria-label="go to the full text of this quotation" rel="noopener" href="https://jack-clark.net/2023/01/16/import-ai-315-generative-antibody-design-rls-imagenet-moment-rl-breaks-rocket-league/#:~:text=A%20standard%20way,with%20malicious%20code." class="quoteback-arrow"> Source</a></div></div>