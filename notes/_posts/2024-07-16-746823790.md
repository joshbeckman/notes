---
title: "Note on GitHub - Exo-Explore/Exo: Run Your Own AI Cluster at Home With Everyday
  Devices \U0001F4F1\U0001F4BB \U0001F5A5️⌚ via GitHub"
tags: composability llm network-theory
canonical: https://github.com/exo-explore/exo
author: GitHub
author_id: d3b7c913cd04ebfec0e9ec32cb6fd58c
book: 42380817
book_title: "GitHub - Exo-Explore/Exo: Run Your Own AI Cluster at Home With Everyday
  Devices \U0001F4F1\U0001F4BB \U0001F5A5️⌚"
hide_title: true
highlight_id: 746823790
readwise_url: https://readwise.io/open/746823790
image: https://readwise-assets.s3.amazonaws.com/media/uploaded_book_covers/profile_265723/exo
favicon_url: https://s2.googleusercontent.com/s2/favicons?domain=github.com
source_emoji: "\U0001F310"
source_url: https://github.com/exo-explore/exo#:~:text=exo%20optimally%20splits,any%20single%20device.
serial_number: 2024.NTS.125
---
> exo optimally splits up models based on the current network topology and device resources available. This enables you to run larger models than you would be able to on any single device.
> <div class="quoteback-footer"><div class="quoteback-avatar"><img class="mini-favicon" src="https://s2.googleusercontent.com/s2/favicons?domain=github.com"></div><div class="quoteback-metadata"><div class="metadata-inner"><span style="display:none">FROM:</span><div aria-label="GitHub" class="quoteback-author"> GitHub</div><div aria-label="GitHub - Exo-Explore/Exo: Run Your Own AI Cluster at Home With Everyday Devices 📱💻 🖥️⌚" class="quoteback-title"> GitHub - Exo-Explore/Exo: Run Your Own AI Cluster at Home With Everyday Devices 📱💻 🖥️⌚</div></div></div><div class="quoteback-backlink"><a target="_blank" aria-label="go to the full text of this quotation" rel="noopener" href="https://github.com/exo-explore/exo#:~:text=exo%20optimally%20splits,any%20single%20device." class="quoteback-arrow"> Source</a></div></div>

The embeddings for Llama-3-8B are around 8KB-10KB. For Llama-3-70B they're around 32KB. These are small enough to send around between devices on a local network.

This kind of swarm compute is so cool and should be more common. Definitely gets us closer to [frugal](https://www.joshbeckman.org/notes/454947468) and [salvage](https://www.joshbeckman.org/notes/454947480) computing and permacomputing.

[AIHorde](https://aihorde.net/) is another example (but using peer compute).