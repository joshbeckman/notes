---
title: "In many ways, this shouldn’t be a surprise to anyone. The cu..."
tags: llm open-source
canonical: https://www.semianalysis.com/p/google-we-have-no-moat-and-neither
author: Dylan Patel
book: 27295974
book_title: "Google 'We Have No Moat, and Neither Does OpenAI'"
book_asin: 
hide_title: true
readwise_url: https://readwise.io/open/522476594
image: https://readwise-assets.s3.amazonaws.com/media/uploaded_book_covers/profile_265723/https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpub_QucYOCv.png
favicon_url: https://s2.googleusercontent.com/s2/favicons?domain=www.semianalysis.com
source_emoji: 🌐
source_url: "https://www.semianalysis.com/p/google-we-have-no-moat-and-neither#:~:text=In%20many%20ways%2C,are%20the%20same."
---

> In many ways, this shouldn’t be a surprise to anyone. The current renaissance in open source LLMs comes hot on the heels of a renaissance in image generation. The similarities are not lost on the community, with many calling this the “[Stable Diffusion moment](https://simonwillison.net/2023/Mar/11/llama/)” for LLMs.
> 
> In both cases, low-cost public involvement was enabled by a vastly cheaper mechanism for fine tuning called [low rank adaptation](https://arxiv.org/abs/2106.09685), or LoRA, combined with a significant breakthrough in scale ([latent diffusion](https://arxiv.org/abs/2112.10752) for image synthesis, [Chinchilla](https://arxiv.org/abs/2203.15556) for LLMs). In both cases, access to a sufficiently high-quality model kicked off a flurry of ideas and iteration from individuals and institutions around the world. In both cases, this quickly outpaced the large players.
> 
> These contributions were pivotal in the image generation space, setting Stable Diffusion on a different path from Dall-E. Having an open model led to [product integrations](https://github.com/AbdullahAlfaraj/Auto-Photoshop-StableDiffusion-Plugin), [marketplaces](https://civitai.com/), [user interfaces](https://github.com/AUTOMATIC1111/stable-diffusion-webui), and [innovations](https://stablediffusionweb.com/ControlNet) that didn’t happen for Dall-E.
> 
> The effect was palpable: [rapid domination](https://trends.google.com/trends/explore?date=2022-08-01%202023-04-10&q=Stable%20Diffusion,Dall-E&hl=en) in terms of cultural impact vs the OpenAI solution, which became increasingly irrelevant. Whether the same thing will happen for LLMs remains to be seen, but the broad structural elements are the same.
> <div class="quoteback-footer"><div class="quoteback-avatar"><img class="mini-favicon" src="https://s2.googleusercontent.com/s2/favicons?domain=www.semianalysis.com"></div><div class="quoteback-metadata"><div class="metadata-inner"><span style="display:none">FROM:</span><div aria-label="Dylan Patel" class="quoteback-author"> Dylan Patel</div><div aria-label="Google 'We Have No Moat, and Neither Does OpenAI'" class="quoteback-title"> Google 'We Have No Moat, and Neither Does OpenAI'</div></div></div><div class="quoteback-backlink"><a target="_blank" aria-label="go to the full text of this quotation" rel="noopener" href="https://www.semianalysis.com/p/google-we-have-no-moat-and-neither#:~:text=In%20many%20ways%2C,are%20the%20same." class="quoteback-arrow"> Source</a></div></div>

So far, we haven't seen quite the same dominance of open-weight-LLMs as we did with image generators/transformers...