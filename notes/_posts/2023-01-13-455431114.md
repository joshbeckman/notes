---
title: "Note on How We Could Stumble Into AI Catastrophe via Cold Takes"
tags: ai alignment
canonical: https://www.cold-takes.com/how-we-could-stumble-into-ai-catastrophe/
author: Cold Takes
author_id: 1a2434952e956181
book: 23226596
book_title: "How We Could Stumble Into AI Catastrophe"
hide_title: true
highlight_id: 455431114
readwise_url: https://readwise.io/open/455431114
image: https://www.cold-takes.com/content/images/2023/01/wile-c-coyote-twitter.png
favicon_url: https://s2.googleusercontent.com/s2/favicons?domain=www.cold-takes.com
source_emoji: ðŸŒ
source_url: "https://www.cold-takes.com/how-we-could-stumble-into-ai-catastrophe/#:~:text=The%20**lab%20mice,on%20lab%20mice."
---

> The **lab mice problem** **Today's "subhuman" AIs are safe.**What about **future AIs with more human-like abilities?**
> 
> Today's AI systems aren't advanced enough to exhibit the basic behaviors we want to study, such as deceiving and manipulating humans.
> 
> Like trying to study medicine in humans by experimenting only on lab mice.
> <div class="quoteback-footer"><div class="quoteback-avatar"><img class="mini-favicon" src="https://s2.googleusercontent.com/s2/favicons?domain=www.cold-takes.com"></div><div class="quoteback-metadata"><div class="metadata-inner"><span style="display:none">FROM:</span><div aria-label="Cold Takes" class="quoteback-author"> Cold Takes</div><div aria-label="How We Could Stumble Into AI Catastrophe" class="quoteback-title"> How We Could Stumble Into AI Catastrophe</div></div></div><div class="quoteback-backlink"><a target="_blank" aria-label="go to the full text of this quotation" rel="noopener" href="https://www.cold-takes.com/how-we-could-stumble-into-ai-catastrophe/#:~:text=The%20**lab%20mice,on%20lab%20mice." class="quoteback-arrow"> Source</a></div></div>