---
title: Note on Is the LLM Response Wrong, or Have You Just Failed to Iterate It? via
  Mike Caulfield
tags: communication llm research tools
canonical: https://mikecaulfield.substack.com/p/is-the-llm-response-wrong-or-have
author: Mike Caulfield
author_id: 9f198773e64271524f32dd0549f758ba
book: 54887083
book_title: Is the LLM Response Wrong, or Have You Just Failed to Iterate It?
hide_title: true
highlight_id: 936274709
readwise_url: https://readwise.io/open/936274709
image: https://substackcdn.com/image/fetch/$s_!oNMw!,w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46d41dd1-84e3-4dff-ac73-2c74457eabe3_791x1047.png
favicon_url: https://s2.googleusercontent.com/s2/favicons?domain=mikecaulfield.substack.com
source_emoji: "\U0001F310"
source_url: https://mikecaulfield.substack.com/p/is-the-llm-response-wrong-or-have#:~:text=I%20call%20these,and%20%E2%80%9Cold%E2%80%9D%20buckets*%29
serial_number: 2025.NTE.134
---
> I call these “sorting” prompts because they usually push the system to go out and try to find things for each “bucket” rather than support a single side of an issue or argue against it. I keep these for myself in a “follow-ups” file. Here are some of the prompts in my file:
> 
> • Read the room: what do a variety of experts think about the claim? How does scientific, professional, popular, and media coverage break down and what does that breakdown tell us?
> 
> • Facts and misconceptions about what I posted
> 
> • Facts and misconceptions and hype about what I posted (*Note: good for health claims in particular*)
> 
> • What is the evidence for and against the claim I posted
> 
> • Look at the most recent information on this issue, summarize how it shifts the analysis (if at all), and provide link to the latest info (*Note: I consider this a sorting prompt because it pushes the system to put evidence of “new” and “old” buckets*)
> <div class="quoteback-footer"><div class="quoteback-avatar"><img class="mini-favicon" src="https://s2.googleusercontent.com/s2/favicons?domain=mikecaulfield.substack.com"></div><div class="quoteback-metadata"><div class="metadata-inner"><span style="display:none">FROM:</span><div aria-label="Mike Caulfield" class="quoteback-author"> Mike Caulfield</div><div aria-label="Is the LLM Response Wrong, or Have You Just Failed to Iterate It?" class="quoteback-title"> Is the LLM Response Wrong, or Have You Just Failed to Iterate It?</div></div></div><div class="quoteback-backlink"><a target="_blank" aria-label="go to the full text of this quotation" rel="noopener" href="https://mikecaulfield.substack.com/p/is-the-llm-response-wrong-or-have#:~:text=I%20call%20these,and%20%E2%80%9Cold%E2%80%9D%20buckets*%29" class="quoteback-arrow"> Source</a></div></div>

This post has a really good overview of techniques for getting an LLM (and any human) to actually fact-check claims and go deeper than a cursory response.

I actually extracted a generalized version [into a claude slash command](https://github.com/joshbeckman/dotfiles/blob/master/.claude/commands/claim-evidence-sort.md).