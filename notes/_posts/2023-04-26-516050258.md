---
title: Note on Double Descent in Human Learning via chris-said.io
tags: machine-learning data
canonical: https://chris-said.io/2023/04/21/double-descent-in-human-learning/
author: chris-said.io
author_id: 6d9c3ec6edb420f0ad6ebf2e8f88f05a
book: 26912620
book_title: Double Descent in Human Learning
hide_title: true
highlight_id: 516050258
readwise_url: https://readwise.io/open/516050258
image: https://chris-said.io/assets/2023_double_descent/fig_preview.png
favicon_url: https://s2.googleusercontent.com/s2/favicons?domain=chris-said.io
source_emoji: "\U0001F310"
source_url: https://chris-said.io/2023/04/21/double-descent-in-human-learning/#:~:text=The%20way%20double,to%20training%20samples.
serial_number: 2023.NTE.428
---
> The way double descent is normally presented, increasing the number of model parameters can make performance worse before it gets better. But there is another even more shocking phenomenon called *data double descent*, where increasing the number of *training samples* can cause performance to get worse before it gets better. These two phenomena are essentially mirror images of each other. Thatâ€™s because the explosion in test error depends on the ratio of parameters to training samples.
> <div class="quoteback-footer"><div class="quoteback-avatar"><img class="mini-favicon" src="https://s2.googleusercontent.com/s2/favicons?domain=chris-said.io"></div><div class="quoteback-metadata"><div class="metadata-inner"><span style="display:none">FROM:</span><div aria-label="chris-said.io" class="quoteback-author"> chris-said.io</div><div aria-label="Double Descent in Human Learning" class="quoteback-title"> Double Descent in Human Learning</div></div></div><div class="quoteback-backlink"><a target="_blank" aria-label="go to the full text of this quotation" rel="noopener" href="https://chris-said.io/2023/04/21/double-descent-in-human-learning/#:~:text=The%20way%20double,to%20training%20samples." class="quoteback-arrow"> Source</a></div></div>