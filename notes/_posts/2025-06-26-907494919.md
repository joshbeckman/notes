---
title: "Note on The Lethal Trifecta for AI Agents: Private Data, Untrusted Content, and External Communication via Simon Willison"
tags: llm safety security
canonical: https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/
author: Simon Willison
author_id: fa3dbb345538cbaf169deab8a01cc6aa
book: 52805352
book_title: "The Lethal Trifecta for AI Agents: Private Data, Untrusted Content, and External Communication"
hide_title: true
highlight_id: 907494919
readwise_url: https://readwise.io/open/907494919
image: https://static.simonwillison.net/static/2025/lethaltrifecta.jpg
favicon_url: https://s2.googleusercontent.com/s2/favicons?domain=simonwillison.net
source_emoji: ðŸŒ
source_url: "https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/#:~:text=If%20you%20are,instructions%20in%20content"
---

> If you are a user of LLM systems that use tools (you can call them â€œAI agentsâ€ if you like) it is *critically* important that you understand the risk of combining tools with the following three characteristics. Failing to understand this **can let an attacker steal your data**.
> 
> The **lethal trifecta** of capabilities is:
> 
> - **Access to your private data**â€”one of the most common purposes of tools in the first place!
> - **Exposure to untrusted content**â€”any mechanism by which text (or images) controlled by a malicious attacker could become available to your LLM
> - **The ability to externally communicate** in a way that could be used to steal your data (I often call this â€œexfiltrationâ€ but Iâ€™m not confident that term is widely understood.)
> 
> If your agent combines these three features, an attacker can **easily trick it** into accessing your private data and sending it to that attacker.
> 
> ![.](https://static.simonwillison.net/static/2025/lethaltrifecta.jpg)
> 
> The problem is that LLMs follow instructions in content
> <div class="quoteback-footer"><div class="quoteback-avatar"><img class="mini-favicon" src="https://s2.googleusercontent.com/s2/favicons?domain=simonwillison.net"></div><div class="quoteback-metadata"><div class="metadata-inner"><span style="display:none">FROM:</span><div aria-label="Simon Willison" class="quoteback-author"> Simon Willison</div><div aria-label="The Lethal Trifecta for AI Agents: Private Data, Untrusted Content, and External Communication" class="quoteback-title"> The Lethal Trifecta for AI Agents: Private Data, Untrusted Content, and External Communication</div></div></div><div class="quoteback-backlink"><a target="_blank" aria-label="go to the full text of this quotation" rel="noopener" href="https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/#:~:text=If%20you%20are,instructions%20in%20content" class="quoteback-arrow"> Source</a></div></div>

I think of it as: every message from a user and response from a tool call is exogenous code
Reminds me to think about [the SUX rule](https://www.joshbeckman.org/notes/608689674) to prevent untrusted external code (which is just plain/natural language, in an LLM system) from being executed outside of a sandbox.