---
title: "recognizing this difference between base models and feedback..."
tags: side-effects rlhf ai dpo machine-learning alignment llm epistemology
canonical: https://thesephist.com/posts/epistemic-calibration/?utm_source=thesephist&utm_medium=email&utm_campaign=environments-for-thinking
author: thesephist.com
book: 42364804
book_title: "Epistemic Calibration and Searching the Space of Truth"
book_asin: 
hide_title: true
readwise_url: https://readwise.io/open/746573875
image: https://readwise-assets.s3.amazonaws.com/static/images/article0.00998d930354.png
favicon_url: https://s2.googleusercontent.com/s2/favicons?domain=thesephist.com
source_emoji: ðŸŒ
source_url: "https://thesephist.com/posts/epistemic-calibration/?utm_source=thesephist&utm_medium=email&utm_campaign=environments-for-thinking#:~:text=recognizing%20this%20difference,from%20a%20reviewer."
---

> recognizing this difference between base models and feedback-tuned models is important, because this kind of a preference tuning step changes what the model is doing at a fundamental level. A pretrained base model is an **epistemically calibrated world model**. Itâ€™s **epistemically calibrated**, meaning its output probabilities exactly mirror frequency of concepts and styles present in its training dataset. If 2% of all photos of waterfalls also have rainbows, exactly 2% of photos of waterfalls the model generates will have rainbows. Itâ€™s also a **world model**, in the sense that what results from pretraining is a probabilistic model of observations of the world (its training dataset). Anything we can find in the training dataset, we can also expect to find in the modelâ€™s output space.
> 
> Once we subject the model to preference tuning, however, the model transforms into something very different, a function that greedily and cleverly finds a way to interpret every input into a version of the request that includes elements it knows is most likely to result in a positive rating from a reviewer.
> <div class="quoteback-footer"><div class="quoteback-avatar"><img class="mini-favicon" src="https://s2.googleusercontent.com/s2/favicons?domain=thesephist.com"></div><div class="quoteback-metadata"><div class="metadata-inner"><span style="display:none">FROM:</span><div aria-label="thesephist.com" class="quoteback-author"> thesephist.com</div><div aria-label="Epistemic Calibration and Searching the Space of Truth" class="quoteback-title"> Epistemic Calibration and Searching the Space of Truth</div></div></div><div class="quoteback-backlink"><a target="_blank" aria-label="go to the full text of this quotation" rel="noopener" href="https://thesephist.com/posts/epistemic-calibration/?utm_source=thesephist&utm_medium=email&utm_campaign=environments-for-thinking#:~:text=recognizing%20this%20difference,from%20a%20reviewer." class="quoteback-arrow"> Source</a></div></div>

Preference tuning methods like RLHF and DPO change the goal of a large neural net model: from modeling the input to targeting approval.