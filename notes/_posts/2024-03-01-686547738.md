---
title: "Moore's law has this idea that, like, computers for a long t..."
tags: optimization machine-learning scalability data
canonical: https://www.youtube.com/watch?v=pdJQ8iVTwj8
author: Lex Fridman
book: 38270867
book_title: "Chris Lattner: Future of Programming and AI | Lex Fridman Podcast #381"
book_asin: 
hide_title: true
readwise_url: https://readwise.io/open/686547738
image: https://i1.ytimg.com/vi/pdJQ8iVTwj8/hqdefault.jpg
favicon_url: https://s2.googleusercontent.com/s2/favicons?domain=www.youtube.com
source_emoji: 🌐
source_url: "https://www.youtube.com/watch?v=pdJQ8iVTwj8#:~:text=Moore%27s%20law%20has,all%20the%20stuff."
---

> Moore's law has this idea that, like, computers for a long time - single thread performance just got faster and faster and faster and faster for free. But then physics and other things intervened - in power consumption - like other things started to matter.
> 
> And so what ended up happening is we went from single core computers to multi-core. Then we went to accelerators, right, this, this trend towards specialization of hardware is only going to continue. And so for years us programming language nerds and compiler people have been saying, "okay, well how do we tackle multi-core," right? For a while it was like "multi-core is the future, we have to get on top of this thing," and then it was "multi-core is the default, what are we doing with this thing" and then it's like "there's chips with hundreds of cores in them what happened," right?
> 
> So I'm super inspired by the fact that, you know, in the face of this, you know, those machine learning people invented this idea of a tensor, right? [[And a tensor is a, like, an arithmetic and algebraic concept. It's like an abstraction around a gigantic parallelizable data set, right?::highlight]] And because of that and because of things like Tensorflow and PyTorch we're able to say: okay, well, [[express _the math_ of the system. This enables you to do automatic differentiations, enables you to, like, all these cool things. And it's an abstract representation and because you have that abstract representation you can now map it onto these parallel machines::highlight]] without having to control "okay, put that right here, put that right there, put that right there." And this has enabled an explosion in terms of AI compute accelerators - like all the stuff.
> <div class="quoteback-footer"><div class="quoteback-avatar"><img class="mini-favicon" src="https://s2.googleusercontent.com/s2/favicons?domain=www.youtube.com"></div><div class="quoteback-metadata"><div class="metadata-inner"><span style="display:none">FROM:</span><div aria-label="Lex Fridman" class="quoteback-author"> Lex Fridman</div><div aria-label="Chris Lattner: Future of Programming and AI | Lex Fridman Podcast #381" class="quoteback-title"> Chris Lattner: Future of Programming and AI | Lex Fridman Podcast #381</div></div></div><div class="quoteback-backlink"><a target="_blank" aria-label="go to the full text of this quotation" rel="noopener" href="https://www.youtube.com/watch?v=pdJQ8iVTwj8#:~:text=Moore%27s%20law%20has,all%20the%20stuff." class="quoteback-arrow"> Source</a></div></div>

_(~1:02 in the recording)_

There's so much going on in this stream of thought.

This idea that matrices/tensors are an innately parallel data structure that enables parallel computing (in GPUs or across machines) is very interesting.