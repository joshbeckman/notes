---
title: "Use a small model to generate a 'draft' output, then use a l..."
tags: ai performance scalability
canonical: mailto:reader-forwarded-email/3fed8d021c102ea6eb0fd341ab68ebfd
author: Josh Beckman
book: 24129773
book_title: "Fwd: Import AI 317: DeepMind Speeds Up Language Model Sampling; Voice Cloning Tech Gets Abused; More Scaling Laws for RL"
book_asin: 
hide_title: true
readwise_url: https://readwise.io/open/471678563
image: https://readwise-assets.s3.amazonaws.com/static/images/article4.6bc1851654a0.png
favicon_url: 
source_emoji: ✉️
---

> Use a small model to generate a 'draft' output, then use a larger and smarter model to score the 'draft', then use a rejection sampling scheme to accept the tokens which are agreed by the small and large models.  
> 
> In tests, they find that a draft model can give them speedups ranging between 1.92X  (on a summarization benchmark called XSum) and 2.46X on a code generation task called HumanEval.
> <div class="quoteback-footer"><div class="quoteback-avatar"><span class="mini-emoji"> ✉️</span></div><div class="quoteback-metadata"><div class="metadata-inner"><span style="display:none">FROM:</span><div aria-label="Josh Beckman" class="quoteback-author"> Josh Beckman</div><div aria-label="Fwd: Import AI 317: DeepMind Speeds Up Language Model Sampling; Voice Cloning Tech Gets Abused; More Scaling Laws for RL" class="quoteback-title"> Fwd: Import AI 317: DeepMind Speeds Up Language Model Sampling; Voice Cloning Tech Gets Abused; More Scaling Laws for RL</div></div></div></div>