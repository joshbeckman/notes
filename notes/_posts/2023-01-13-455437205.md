---
title: Note on How We Could Stumble Into AI Catastrophe via Cold Takes
tags: ai incentives alignment
canonical: https://www.cold-takes.com/how-we-could-stumble-into-ai-catastrophe/
author: Cold Takes
author_id: ce4c0db4e36209320900cfde241e9477
book: 23226596
book_title: How We Could Stumble Into AI Catastrophe
hide_title: true
highlight_id: 455437205
readwise_url: https://readwise.io/open/455437205
image: https://www.cold-takes.com/content/images/2023/01/wile-c-coyote-twitter.png
favicon_url: https://s2.googleusercontent.com/s2/favicons?domain=www.cold-takes.com
source_emoji: "\U0001F310"
source_url: https://www.cold-takes.com/how-we-could-stumble-into-ai-catastrophe/#:~:text=Maybe%20they%E2%80%99re%20just,into%20certain%20states%29.
serial_number: 2023.NTE.066
---
> Maybe they’re just creating massive amounts of “digital representations of human approval,” because this is what they were historically trained to seek (kind of like how humans sometimes do whatever it takes to get drugs that will get their brains into certain states).
> <div class="quoteback-footer"><div class="quoteback-avatar"><img class="mini-favicon" src="https://s2.googleusercontent.com/s2/favicons?domain=www.cold-takes.com"></div><div class="quoteback-metadata"><div class="metadata-inner"><span style="display:none">FROM:</span><div aria-label="Cold Takes" class="quoteback-author"> Cold Takes</div><div aria-label="How We Could Stumble Into AI Catastrophe" class="quoteback-title"> How We Could Stumble Into AI Catastrophe</div></div></div><div class="quoteback-backlink"><a target="_blank" aria-label="go to the full text of this quotation" rel="noopener" href="https://www.cold-takes.com/how-we-could-stumble-into-ai-catastrophe/#:~:text=Maybe%20they%E2%80%99re%20just,into%20certain%20states%29." class="quoteback-arrow"> Source</a></div></div>