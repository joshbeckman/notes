---
title: 'Note on S1: The $6 R1 Competitor? via Tim Kellogg'
tags: llm feedback
canonical: https://timkellogg.me/blog/2025/02/03/s1
author: Tim Kellogg
author_id: ac47cae2498b9be0dc769a6f616dc878
book: 48505656
book_title: 'S1: The $6 R1 Competitor?'
hide_title: true
highlight_id: 848621117
readwise_url: https://readwise.io/open/848621117
image: https://cdn.pixabay.com/photo/2024/08/13/16/50/ai-generated-8966531_960_720.png
favicon_url: https://s2.googleusercontent.com/s2/favicons?domain=timkellogg.me
source_emoji: "\U0001F310"
source_url: https://timkellogg.me/blog/2025/02/03/s1#:~:text=In%20s1%2C%20when,I%20love%20it.
serial_number: 2025.NTS.022
---
> In s1, when the LLM tries to stop thinking with `"</think>"`, they force it to keep going by replacing it with `"Wait"`. It’ll then begin to second guess and double check it’s answer. They do this to trim or extend thinking time (trimming is just abruptly inserting `"</think>"`).
> 
> It’s really dumb, I love it.
> <div class="quoteback-footer"><div class="quoteback-avatar"><img class="mini-favicon" src="https://s2.googleusercontent.com/s2/favicons?domain=timkellogg.me"></div><div class="quoteback-metadata"><div class="metadata-inner"><span style="display:none">FROM:</span><div aria-label="Tim Kellogg" class="quoteback-author"> Tim Kellogg</div><div aria-label="S1: The $6 R1 Competitor?" class="quoteback-title"> S1: The $6 R1 Competitor?</div></div></div><div class="quoteback-backlink"><a target="_blank" aria-label="go to the full text of this quotation" rel="noopener" href="https://timkellogg.me/blog/2025/02/03/s1#:~:text=In%20s1%2C%20when,I%20love%20it." class="quoteback-arrow"> Source</a></div></div>

I did this to myself today by typing out a full response to a colleague then stepping back and forcing myself to rethink it.

There are so many simple tricks still to be discovered with LLMs: here, an example of SFT (supervised fine tuning) over RLHF (reinforcement learning human feedback).