---
title: "Note on The Surprising Ease and Effectiveness of Looping AI via Matt Webb"
tags: ai llm
canonical: https://interconnected.org/home/2023/03/16/singularity
author: Matt Webb
author_id: 18b4c4044835c8dd2782e764b40b5441
book: 27435006
book_title: "The Surprising Ease and Effectiveness of Looping AI"
hide_title: true
highlight_id: 524990338
readwise_url: https://readwise.io/open/524990338
image: https://interconnected.org/home/static/images/matt-webb-profile-square-small.jpg?v=1
favicon_url: https://s2.googleusercontent.com/s2/favicons?domain=interconnected.org
source_emoji: 🌐
source_url: "https://interconnected.org/home/2023/03/16/singularity#:~:text=the%20tech%20am,to%20factual%20sources."
---

> the tech am I digging recently is a software framework called **LangChain** ([here are the docs](https://langchain.readthedocs.io/en/latest/)) which does something pretty straightforward: it makes it easy to call OpenAI’s GPT, say, a dozen times in a loop to answer a single question, and mix in queries to Wikipedia and other databases.
> 
> This is a big deal because of a technique called **ReAct** from a paper out of Princeton and Google Research *([the ReAct website](https://react-lm.github.io) links to the Nov 2022 paper, sample code, etc).*
> 
> ReAct looks innocuous but here’s the deal: instead of asking GPT to simply do smart-autocomplete on your text, you prompt it to respond in a thought/act/observation loop. So you ask GPT to respond like:
> 
> > Thought: Let’s think step by step. I need to find out X and then do Y.
> > 
> > Act: Search Wikipedia for X
> > 
> > Observation: From the Wikipedia page I have learnt that …
> > 
> > Thought: So the answer is …
> 
> And it is allowed to repeat as many times as necessarily, iterating towards its goal.
> 
> The clever bit is that, using LangChain, you *intercept* GPT when it starts a line with *“Act:”* and then you go and do that action for it, feeding the results back in as an *“Observation”* line so that it can “think” what to do next.
> 
> The *really* clever bit is that, at the outset, you tell GPT what tools it has available, and how to access them. So it might have:
> 
> •   Public databases like Wikipedia or IMDB or arXiv or company registers
> •   Proprietary databases like your internal HR system
> •   One-shot tools like a calculator, or a programming language
> •   Systems it can drive, not just query – like it could open and close windows on your computer, if you built an interface, or trundle a robot forward for a better view.
> 
> And this is *wild.*
> 
> Because now we have reasoning, goal-directed action, and *tool use* for AI.
> 
> It circumvents the problem of the language model “lying” (LLMs tend to be highly convincing confabulators) by giving it access to factual sources.
> <div class="quoteback-footer"><div class="quoteback-avatar"><img class="mini-favicon" src="https://s2.googleusercontent.com/s2/favicons?domain=interconnected.org"></div><div class="quoteback-metadata"><div class="metadata-inner"><span style="display:none">FROM:</span><div aria-label="Matt Webb" class="quoteback-author"> Matt Webb</div><div aria-label="The Surprising Ease and Effectiveness of Looping AI" class="quoteback-title"> The Surprising Ease and Effectiveness of Looping AI</div></div></div><div class="quoteback-backlink"><a target="_blank" aria-label="go to the full text of this quotation" rel="noopener" href="https://interconnected.org/home/2023/03/16/singularity#:~:text=the%20tech%20am,to%20factual%20sources." class="quoteback-arrow"> Source</a></div></div>