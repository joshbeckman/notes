---
title: "Note on This Is Paper Is Kinda Wild via gian 🏴‍☠️"
tags: llm
canonical: https://x.com/giansegato/status/1869540774519730553
author: gian 🏴‍☠️
author_id: 0d34c044583342903f67f5082ac651b6
book: 46961810
book_title: "This Is Paper Is Kinda Wild"
hide_title: true
highlight_id: 826903946
readwise_url: https://readwise.io/open/826903946
image: https://pbs.twimg.com/profile_images/1791960549221146624/nbXUVS9q.jpg
favicon_url: https://s2.googleusercontent.com/s2/favicons?domain=x.com
source_emoji: 🌐
source_url: "https://x.com/giansegato/status/1869540774519730553#:~:text=kinda%20wild.%20turns,models%20%28eg%2C%20time-LLM%29"
---

> kinda wild. turns out that if you simply ask an LLM to straight out predict a timeseries like this:
> 
> ```  
> <history>  
> (t1, v1) (t2, v2) (t3, v3)  
> </history>  
> <forecast>  
> (t4, v4) (t5, v5)  
> </forecast>  
> ```
> 
> making sure to prepend the prompt like this:
> 
> ```  
> Here is some context about the task. Make sure to factor in any background knowledge, satisfy any constraints, and respect any scenarios.  
> <context>  
> ((context))  
> </context>  
> ```
> 
> it will just… do it? beating SOTA timeseries forcasting?!
> 
> llama 3.1 405b directly prompted is more precise at forecasting real-world series than:  
> - stats-based timeseries models (ARIMA, ETS)  
> - foundation models specifically trained for time series (eg. chronos)  
> - multimodal forecasting models (eg, time-LLM)
> <div class="quoteback-footer"><div class="quoteback-avatar"><img class="mini-favicon" src="https://s2.googleusercontent.com/s2/favicons?domain=x.com"></div><div class="quoteback-metadata"><div class="metadata-inner"><span style="display:none">FROM:</span><div aria-label="gian 🏴‍☠️" class="quoteback-author"> gian 🏴‍☠️</div><div aria-label="This Is Paper Is Kinda Wild" class="quoteback-title"> This Is Paper Is Kinda Wild</div></div></div><div class="quoteback-backlink"><a target="_blank" aria-label="go to the full text of this quotation" rel="noopener" href="https://x.com/giansegato/status/1869540774519730553#:~:text=kinda%20wild.%20turns,models%20%28eg%2C%20time-LLM%29" class="quoteback-arrow"> Source</a></div></div>

The [bitter lesson](https://www.joshbeckman.org/notes/786450120) strikes again. Searching the space/context is better than an algorithm.