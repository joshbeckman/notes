---
title: 'Note on User Action Sequence Modeling: From Attention to Transformers and
  Beyond via Samuel Flender'
tags: ai
canonical: https://mlfrontiers.substack.com/p/user-action-sequence-modeling-from
author: Samuel Flender
author_id: b38caf493bd11d0a28698dac8d477a70
book: 42914604
book_title: 'User Action Sequence Modeling: From Attention to Transformers and Beyond'
hide_title: true
highlight_id: 754616770
readwise_url: https://readwise.io/open/754616770
image: https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66c18df1-75fc-4807-b2f0-29e821d56eda_1024x1024.webp
favicon_url: https://s2.googleusercontent.com/s2/favicons?domain=mlfrontiers.substack.com
source_emoji: "\U0001F310"
source_url: https://mlfrontiers.substack.com/p/user-action-sequence-modeling-from#:~:text=Another%20key%20distinction,is%20constantly%20evolving.
serial_number: 2024.NTE.138
---
> Another key distinction to Transformers is that the attention scores A(X) in the HSTU block are not normalized using softmax. We’ve seen this in DIN, and the reasoning here is the same: we want to the model to be sensitive to the intensity of inputs, that is, the total count of user actions, instead of just their relative frequencies. The underlying reason is that, unlike in LLMs, the corpus of tokens (i.e. ids) is not stationary but instead rapidly evolving, with new tokens constantly being introduced into and old tokens vanishing from the corpus. Softmax does not work well if the space it is normalizing over is constantly evolving.
> <div class="quoteback-footer"><div class="quoteback-avatar"><img class="mini-favicon" src="https://s2.googleusercontent.com/s2/favicons?domain=mlfrontiers.substack.com"></div><div class="quoteback-metadata"><div class="metadata-inner"><span style="display:none">FROM:</span><div aria-label="Samuel Flender" class="quoteback-author"> Samuel Flender</div><div aria-label="User Action Sequence Modeling: From Attention to Transformers and Beyond" class="quoteback-title"> User Action Sequence Modeling: From Attention to Transformers and Beyond</div></div></div><div class="quoteback-backlink"><a target="_blank" aria-label="go to the full text of this quotation" rel="noopener" href="https://mlfrontiers.substack.com/p/user-action-sequence-modeling-from#:~:text=Another%20key%20distinction,is%20constantly%20evolving." class="quoteback-arrow"> Source</a></div></div>

Meta’s HSTU, short for Hierarchical Sequential Transduction Units