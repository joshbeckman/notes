---
title: "Note on AI CoT Reasoning Is Often Unfaithful via Zvi Mowshowitz"
tags: safety llm
canonical: https://thezvi.substack.com/p/ai-cot-reasoning-is-often-unfaithful
author: Zvi Mowshowitz
author_id: 547e7c9bdf91e2949061d32017f66d68
book: 50339835
book_title: "AI CoT Reasoning Is Often Unfaithful"
hide_title: true
highlight_id: 874223093
readwise_url: https://readwise.io/open/874223093
image: https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F572881ac-4f69-48b7-8b39-01b277b9297a_689x421.png
favicon_url: https://s2.googleusercontent.com/s2/favicons?domain=thezvi.substack.com
source_emoji: 🌐
source_url: "https://thezvi.substack.com/p/ai-cot-reasoning-is-often-unfaithful#:~:text=It%20means%20the,more%20of%20it."
---

> It means the reasoning does not predict the output. That’s it. I would also once again say that deception and obfuscation are not distinct magisteria, and that all of this is happening for Janus-compatible reasons.
> 
> It’s not that AIs sometimes do things ‘on purpose’ and other times they do things ‘not on purpose,’ let alone that the ‘not on purpose’ means there’s nothing to worry about. It would still mean you can’t rely on the CoT, which is all Anthropic is warning about.
> 
> It’s not the same concept, but I notice the same applies to ‘unfaithful’ in other senses as well. If someone is not ‘intentionally’ unfaithful in the traditional sense, they simply don’t honor their commitments, that still counts.
> 
> What we care about is whether we can rely on the attestations and commitments.
> 
> We now have strong evidence that we cannot do this.
> 
> We cannot even do this for models with no incentive to obfuscate, distort or hide their CoT, and no optimization pressure getting them to do so, on any level.
> 
> The models are doing this by default, likely because it is efficient to do that. It seems likely that more training and more capability will only make it relatively more effective to reason in these non-obvious ways, and we will see even more of it.
> <div class="quoteback-footer"><div class="quoteback-avatar"><img class="mini-favicon" src="https://s2.googleusercontent.com/s2/favicons?domain=thezvi.substack.com"></div><div class="quoteback-metadata"><div class="metadata-inner"><span style="display:none">FROM:</span><div aria-label="Zvi Mowshowitz" class="quoteback-author"> Zvi Mowshowitz</div><div aria-label="AI CoT Reasoning Is Often Unfaithful" class="quoteback-title"> AI CoT Reasoning Is Often Unfaithful</div></div></div><div class="quoteback-backlink"><a target="_blank" aria-label="go to the full text of this quotation" rel="noopener" href="https://thezvi.substack.com/p/ai-cot-reasoning-is-often-unfaithful#:~:text=It%20means%20the,more%20of%20it." class="quoteback-arrow"> Source</a></div></div>

Feels like someone turned on the backstage lights at the AI theater.

> There’s a quiet tension in realizing that even when AI sounds convincing—layered logic, clean rationale—it might just be performing coherence. Not lying, not broken… just stitched-together reasoning that feels true, but isn’t always anchored.