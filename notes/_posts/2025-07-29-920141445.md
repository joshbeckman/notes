---
title: "Note on Context Engineering for AI Agents: Lessons From Building Manus via manus.im"
tags: llm
canonical: https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus
author: manus.im
author_id: a16c532d0210ea7093c95c17624931e3
book: 53759651
book_title: "Context Engineering for AI Agents: Lessons From Building Manus"
hide_title: true
highlight_id: 920141445
readwise_url: https://readwise.io/open/920141445
image: https://files.manuscdn.com/assets/dashboard/materials/2025/07/18/eaafe9e6a174b29458c314ccc225dbdd39a7c9d66e60786235165d9aba23f578.webp
favicon_url: https://s2.googleusercontent.com/s2/favicons?domain=manus.im
source_emoji: 🌐
source_url: "https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus#:~:text=As%20your%20agent,a%20model-driven%20architecture."
---

> As your agent takes on more capabilities, its action space naturally grows more complex—in plain terms, the number of tools explodes. The recent popularity of [MCP](https://modelcontextprotocol.io/introduction) only adds fuel to the fire. If you allow user-configurable tools, trust me: someone will inevitably plug hundreds of mysterious tools into your carefully curated action space. As a result, the model is more likely to select the wrong action or take an inefficient path. In short, your heavily armed agent gets dumber.
> 
> A natural reaction is to design a dynamic action space—perhaps loading tools on demand using something [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation)-like. We tried that in Manus too. But our experiments suggest a clear rule: unless absolutely necessary, avoid dynamically adding or removing tools mid-iteration. There are two main reasons for this:
> 
> 1.In most LLMs, tool definitions live near the front of the context after serialization, typically before or after the system prompt. So any change will invalidate the KV-cache for all subsequent actions and observations.
> 
> 2.When previous actions and observations still refer to tools that are no longer defined in the current context, the model gets confused. Without [constrained decoding](https://platform.openai.com/docs/guides/structured-outputs), this often leads to schema violations or hallucinated actions.
> 
> To solve this while still improving action selection, Manus uses a context-aware [state machine](https://en.wikipedia.org/wiki/Finite-state_machine) to manage tool availability. Rather than removing tools, it masks the token logits during decoding to prevent (or enforce) the selection of certain actions based on the current context.
> 
> ![](https://d1oupeiobkpcny.cloudfront.net/user_upload_by_module/markdown/310708716691272617/cWxINCvUfrmlbvfV.png)
> 
> In practice, most model providers and inference frameworks support some form of response prefill, which allows you to constrain the action space without modifying the tool definitions. There are generally three modes of function calling (we'll use the [Hermes format](https://github.com/NousResearch/Hermes-Function-Calling) from NousResearch as an example):
> 
> •Auto – The model may choose to call a function or not. Implemented by prefilling only the reply prefix: <|im_start|>assistant
> 
> •Required – The model must call a function, but the choice is unconstrained. Implemented by prefilling up to tool call token: <|im_start|>assistant<tool_call>
> 
> •Specified – The model must call a function from a specific subset. Implemented by prefilling up to the beginning of the function name: <|im_start|>assistant<tool_call>{"name": “browser_
> 
> Using this, we constrain action selection by masking token logits directly. For example, when the user provides a new input, Manus must reply immediately instead of taking an action. We've also deliberately designed action names with consistent prefixes—e.g., all browser-related tools start with browser_, and command-line tools with shell_. This allows us to easily enforce that the agent only chooses from a certain group of tools at a given state without using stateful logits processors.
> 
> These designs help ensure that the Manus agent loop remains stable—even under a model-driven architecture.
> <div class="quoteback-footer"><div class="quoteback-avatar"><img class="mini-favicon" src="https://s2.googleusercontent.com/s2/favicons?domain=manus.im"></div><div class="quoteback-metadata"><div class="metadata-inner"><span style="display:none">FROM:</span><div aria-label="manus.im" class="quoteback-author"> manus.im</div><div aria-label="Context Engineering for AI Agents: Lessons From Building Manus" class="quoteback-title"> Context Engineering for AI Agents: Lessons From Building Manus</div></div></div><div class="quoteback-backlink"><a target="_blank" aria-label="go to the full text of this quotation" rel="noopener" href="https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus#:~:text=As%20your%20agent,a%20model-driven%20architecture." class="quoteback-arrow"> Source</a></div></div>

This is a very interesting technique: force an assistant response to every user request, so as to keep open the possibility for removing tools/calls in future edits of the conversation.

Also from this post/overview:
> the file system as the ultimate context