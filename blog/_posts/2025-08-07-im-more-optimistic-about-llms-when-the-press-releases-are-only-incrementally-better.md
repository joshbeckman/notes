---
layout: Post
date: '2025-08-07 02:54:48 +0000'
title: I'm More Optimistic About LLMs When The Press Releases are Only Incrementally
  Better
toc: true
image: "/assets/images/0de7c5d9-ccb5-47b8-8419-5b12c752163b.gif"
description:
mastodon_social_status_url: https://mastodon.social/@joshbeckman/114985892302325909
tags:
- ai
- technology
- programming-languages
- software-engineering
- open-source
- machine-learning
- communication
- research
serial_number: 2025.BLG.124
---
[A coworker friend](https://www.sohanjain.com/) of mine asked me yesterday, "You seem to be publishing a ton recently [on building with LLMs]. Conscious decision to post more? or are you just hacking more?"

Honestly, it's both. I've just gotten so optimistic about what large language/image/video models can do _right now_ and I feel like they're actually _slowing down_ in their progress.

Yesterday, we saw:
- [the latest Anthropic release is a 2% improvement](https://www.anthropic.com/news/claude-opus-4-1)
- [OpenAI are open-sourcing models](https://openai.com/index/introducing-gpt-oss/)
- [Google product releases are light on the implementation details](https://news.ycombinator.com/item?id=44799882)

I'm no astrologer, but that constellation gives me reason to feel that the LLM zeitgeist is shifting from base-model capability growth to good ol' software engineering (data plumbing and tooling and abstractions) skills. I have a _ton_ of ideas on where to push and improve in those directions, because that's what _I'm_ good at. 

There's this scene from one of my all-time favorite TV shows - _[Patriot](https://www.imdb.com/title/tt4687882/)_ - that sums up the shift for me. (And it features my good friend [Andy](http://www.andyjunk.com/) as a one-off character!)

<iframe width="100%" height="350" src="https://www.youtube-nocookie.com/embed/-F-IHvF5OCA?si=8Ex6jhXBs-aBzHiC" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

When all the LLM labs are releasing new step-change-in-capability models every season, with new architectures and behavior patterns, I feel like Andy's character (the one asking the question): struggling to stay afloat, unable to build with confidence on shifting sands. When the major labs only make incremental progress, I feel like Michael Dorman (the main speaker): confident, optimistic, a plumber [guiding the fluid intelligence like water](https://www.joshbeckman.org/notes/735151726).

![We're catching up - from Pirates](/assets/images/0de7c5d9-ccb5-47b8-8419-5b12c752163b.gif)


But who knows, they could release a game-changer tomorrow.
